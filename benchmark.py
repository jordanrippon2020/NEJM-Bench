"""Main benchmark orchestration script."""

import asyncio
import json
import uuid
from datetime import datetime
from pathlib import Path

from config import MODELS, RANDOM_SEED, RESULTS_DIR, SAMPLE_SIZE
from dataset import NEJMDataset, load_and_sample_dataset
from judge import JudgeClient, ModelScores
from models import OpenRouterClient


def generate_report(
    scores: dict[str, ModelScores],
    run_id: str,
    timestamp: str,
    sample_size: int,
) -> str:
    """Generate a markdown report of benchmark results."""
    lines = [
        "# NEJM Image Challenge Benchmark Results",
        "",
        f"**Run ID:** `{run_id}`",
        f"**Timestamp:** {timestamp}",
        f"**Sample Size:** {sample_size} challenges",
        f"**Random Seed:** {RANDOM_SEED}",
        "",
        "---",
        "",
        "## Summary",
        "",
        "| Model | Avg Score | Exact (10) | High (7-9) | Partial (4-6) | Low (1-3) | Wrong (0) |",
        "|-------|-----------|------------|------------|---------------|-----------|-----------|",
    ]

    # Sort models by average score (descending)
    sorted_models = sorted(scores.items(), key=lambda x: x[1].average_score, reverse=True)

    for model_name, model_scores in sorted_models:
        dist = model_scores.score_distribution
        exact = dist.get(10, 0)
        high = sum(dist.get(i, 0) for i in range(7, 10))
        partial = sum(dist.get(i, 0) for i in range(4, 7))
        low = sum(dist.get(i, 0) for i in range(1, 4))
        wrong = dist.get(0, 0)

        lines.append(
            f"| {model_name} | **{model_scores.average_score:.2f}** | {exact} | {high} | {partial} | {low} | {wrong} |"
        )

    lines.extend([
        "",
        "---",
        "",
        "## Score Distribution by Model",
        "",
    ])

    for model_name, model_scores in sorted_models:
        lines.append(f"### {model_name}")
        lines.append("")
        lines.append(f"- **Average Score:** {model_scores.average_score:.2f}/10")
        lines.append(f"- **Successful Evaluations:** {model_scores.success_count}")
        lines.append(f"- **Failed Evaluations:** {model_scores.failure_count}")
        lines.append("")

        # Category breakdown
        lines.append("**Category Breakdown:**")
        for category, count in sorted(model_scores.category_breakdown.items(), key=lambda x: -x[1]):
            lines.append(f"- {category}: {count}")
        lines.append("")

        # Score histogram
        lines.append("**Score Distribution:**")
        lines.append("```")
        dist = model_scores.score_distribution
        max_count = max(dist.values()) if dist.values() else 1
        for score_val in range(10, -1, -1):
            count = dist.get(score_val, 0)
            bar = "â–ˆ" * int(20 * count / max_count) if max_count > 0 else ""
            lines.append(f"{score_val:2d} | {bar} {count}")
        lines.append("```")
        lines.append("")

    lines.extend([
        "---",
        "",
        "## Per-Challenge Details",
        "",
        "| Challenge | Correct Answer | " + " | ".join(m for m, _ in sorted_models) + " |",
        "|-----------|----------------|" + "|".join("---" for _ in sorted_models) + "|",
    ])

    # Get all challenge IDs from the first model's scores
    if sorted_models:
        first_model_scores = sorted_models[0][1].scores
        for i, score_obj in enumerate(first_model_scores):
            challenge_id = score_obj.challenge_id
            correct_answer = score_obj.correct_answer[:40] + "..." if len(score_obj.correct_answer) > 40 else score_obj.correct_answer

            # Get scores for each model for this challenge
            model_score_cells = []
            for model_name, model_scores in sorted_models:
                if i < len(model_scores.scores):
                    s = model_scores.scores[i]
                    model_score_cells.append(f"{s.score:.0f}")
                else:
                    model_score_cells.append("-")

            lines.append(f"| {challenge_id[:15]} | {correct_answer} | " + " | ".join(model_score_cells) + " |")

    lines.extend([
        "",
        "---",
        "",
        "*Generated by NEJM Image Challenge Benchmark*",
    ])

    return "\n".join(lines)


def save_raw_results(
    challenges: list,
    model_responses: dict,
    scores: dict[str, ModelScores],
    run_id: str,
    timestamp: str,
    output_path: Path,
) -> None:
    """Save raw benchmark results as JSON."""
    # Convert to serializable format
    challenges_data = [
        {
            "id": c.id,
            "image_url": c.image_url,
            "clinical_description": c.clinical_description,
            "correct_answer": c.correct_answer,
        }
        for c in challenges
    ]

    responses_data = {}
    for model_name, responses in model_responses.items():
        responses_data[model_name] = [
            {
                "challenge_id": r.challenge_id,
                "response_text": r.response_text,
                "success": r.success,
                "error": r.error,
            }
            for r in responses
        ]

    scores_data = {}
    for model_name, model_scores in scores.items():
        scores_data[model_name] = {
            "average_score": model_scores.average_score,
            "score_distribution": model_scores.score_distribution,
            "category_breakdown": model_scores.category_breakdown,
            "individual_scores": [
                {
                    "challenge_id": s.challenge_id,
                    "score": s.score,
                    "category": s.category,
                    "reasoning": s.reasoning,
                    "model_response": s.model_response,
                    "correct_answer": s.correct_answer,
                    "success": s.success,
                    "error": s.error,
                }
                for s in model_scores.scores
            ],
        }

    output = {
        "run_id": run_id,
        "timestamp": timestamp,
        "config": {
            "sample_size": SAMPLE_SIZE,
            "random_seed": RANDOM_SEED,
            "models": MODELS,
        },
        "challenges": challenges_data,
        "responses": responses_data,
        "scores": scores_data,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2)


async def run_benchmark():
    """Main benchmark execution."""
    run_id = str(uuid.uuid4())[:8]
    timestamp = datetime.now().isoformat()

    print("=" * 60)
    print("NEJM Image Challenge AI Benchmark")
    print("=" * 60)
    print(f"Run ID: {run_id}")
    print(f"Timestamp: {timestamp}")
    print(f"Sample Size: {SAMPLE_SIZE}")
    print(f"Models: {', '.join(MODELS.keys())}")
    print("=" * 60)

    # Step 1: Load and sample dataset
    print("\n[Step 1/4] Loading dataset and sampling challenges...")
    challenges = await load_and_sample_dataset(SAMPLE_SIZE, RANDOM_SEED)
    print(f"Prepared {len(challenges)} challenges with images")

    # Step 2: Query all models
    print("\n[Step 2/4] Querying models...")
    client = OpenRouterClient()
    model_responses = await client.query_all_models(MODELS, challenges)

    # Step 3: Judge all responses
    print("\n[Step 3/4] Judging responses...")
    judge = JudgeClient()
    scores = await judge.judge_all_responses(challenges, model_responses)

    # Step 4: Generate reports
    print("\n[Step 4/4] Generating reports...")
    results_dir = Path(RESULTS_DIR)
    results_dir.mkdir(exist_ok=True)

    # Generate and save markdown report
    report = generate_report(scores, run_id, timestamp, len(challenges))
    report_path = results_dir / f"report_{run_id}.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"Report saved to: {report_path}")

    # Save raw results as JSON
    json_path = results_dir / f"results_{run_id}.json"
    save_raw_results(challenges, model_responses, scores, run_id, timestamp, json_path)
    print(f"Raw results saved to: {json_path}")

    # Print summary
    print("\n" + "=" * 60)
    print("BENCHMARK COMPLETE - SUMMARY")
    print("=" * 60)
    sorted_models = sorted(scores.items(), key=lambda x: x[1].average_score, reverse=True)
    for rank, (model_name, model_scores) in enumerate(sorted_models, 1):
        print(f"{rank}. {model_name}: {model_scores.average_score:.2f}/10")
    print("=" * 60)

    return scores


if __name__ == "__main__":
    asyncio.run(run_benchmark())
